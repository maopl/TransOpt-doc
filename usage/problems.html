<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark Problems &mdash; TransOpt 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=01f34227"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Results Analysis" href="results.html" />
    <link rel="prev" title="Algorithmic objects" href="algorithms.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TransOpt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithmic objects</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Benchmark Problems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#registering-a-new-benchmark-problem">Registering a New Benchmark Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#synthetic-problem">Synthetic Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hyperparameter-optimization-problem">Hyperparameter Optimization Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configurable-software-optimization-problem">Configurable Software Optimization Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rna-inverse-design-problem">RNA Inverse Design Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#protein-inverse-folding-problem">Protein Inverse Folding Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallelization">Parallelization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Using TransOpt via Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/architecture.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TransOpt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Benchmark Problems</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/usage/problems.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="benchmark-problems">
<h1>Benchmark Problems<a class="headerlink" href="#benchmark-problems" title="Link to this heading"></a></h1>
<p>This</p>
<div class="info admonition">
<p class="admonition-title">Overview</p>
<ul class="simple">
<li><p><a class="reference internal" href="#registering-new-problem"><span class="std std-ref">Register</span></a>: How to register a new optimization problem to <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#synthetic-problems"><span class="std std-ref">Synthetic Problem</span></a>: The list of the synthetic problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#hpo-problems"><span class="std std-ref">Hyperparameter Optimization Problem</span></a>: The list of the HPO problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#cso-problems"><span class="std std-ref">Configurable Software Optimization Problem</span></a>: The list of the configurable software optimization problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#rna-problems"><span class="std std-ref">RNA Inverse Design Problem</span></a>: The list of the RNA Inverse design problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#pif-problems"><span class="std std-ref">Protein Inverse Folding Problem</span></a>: The list of the protein inverse folding problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#parallelization"><span class="std std-ref">Parallelization</span></a>: How to parallelize function evaluations</p></li>
</ul>
</div>
<section id="registering-a-new-benchmark-problem">
<span id="registering-new-problem"></span><h2>Registering a New Benchmark Problem<a class="headerlink" href="#registering-a-new-benchmark-problem" title="Link to this heading"></a></h2>
<p>To register a new benchmark problem in the TransOpt framework, follow the steps below.</p>
<p>### 1. Import the Problem Registry</p>
<p>First, you need to import the <cite>problem_registry</cite> from the <cite>transopt.agent.registry</cite> module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transopt.agent.registry</span> <span class="kn">import</span> <span class="n">problem_registry</span>
</pre></div>
</div>
<p>### 2. Define a New Problem Class</p>
<p>Next, define a new problem class. This class should be decorated with the <cite>&#64;problem_registry.register(“ProblemName”)</cite> decorator, where <cite>“ProblemName”</cite> is the unique identifier for the problem. The new problem class must inherit from one of the following base classes:</p>
<ul class="simple">
<li><p><cite>NonTabularProblem</cite></p></li>
<li><p><cite>TabularProblem</cite></p></li>
</ul>
<p>For example, to create a new problem named “new_problem”, you would define the class as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@problem_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;new_problem&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">new_problem</span><span class="p">(</span><span class="n">NonTabularProblem</span><span class="p">):</span>
    <span class="k">pass</span>  <span class="c1"># Further implementation required</span>
</pre></div>
</div>
<p>### 3. Implement Required Methods</p>
<p>After defining the class, you need to implement the following three abstract methods:</p>
<ol class="arabic">
<li><p><strong>get_configuration_space</strong>:
This method is responsible for defining the configuration space of the new problem.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_configuration_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Define and return the configuration space</span>
    <span class="k">pass</span>
</pre></div>
</div>
</li>
<li><p><strong>get_fidelity_space</strong>:
This method should define the fidelity space for the problem, if applicable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_fidelity_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Define and return the fidelity space</span>
    <span class="k">pass</span>
</pre></div>
</div>
</li>
<li><p><strong>objective_function</strong>:
This method evaluates the problem’s objective function based on the provided configuration and other parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">configuration</span><span class="p">,</span> <span class="n">fidelity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="c1"># Evaluate the configuration and return the results as a dictionary</span>
    <span class="k">pass</span>
</pre></div>
</div>
</li>
</ol>
<p>Here’s an example outline of the <cite>sphere</cite> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@problem_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;sphere&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">sphere</span><span class="p">(</span><span class="n">NonTabularProblem</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">get_configuration_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Define the configuration space here</span>
     <span class="n">variables</span> <span class="o">=</span>  <span class="p">[</span><span class="n">Continuous</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mf">5.12</span><span class="p">,</span> <span class="mf">5.12</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)]</span>
     <span class="n">ss</span> <span class="o">=</span> <span class="n">SearchSpace</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">ss</span>

  <span class="k">def</span> <span class="nf">get_fidelity_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FidelitySpace</span><span class="p">:</span>
     <span class="n">fs</span> <span class="o">=</span> <span class="n">FidelitySpace</span><span class="p">([])</span>
     <span class="k">return</span> <span class="n">fs</span>

  <span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">configuration</span><span class="p">,</span> <span class="n">fidelity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
     <span class="c1"># Implement the evaluation logic and return the results as a dictionary</span>
     <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">configuration</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">configuration</span><span class="o">.</span><span class="n">keys</span><span class="p">())]])</span>
     <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
     <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;function_value&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span>

     <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>By following these steps, you can successfully register a new benchmark problem in the TransOpt framework.</p>
</section>
<section id="synthetic-problem">
<span id="synthetic-problems"></span><h2>Synthetic Problem<a class="headerlink" href="#synthetic-problem" title="Link to this heading"></a></h2>
<p>The synthetic problems in this section are widely used in the optimization literature for benchmarking optimization algorithms. These problems exhibit diverse characteristics and levels of complexity, making them ideal for testing the robustness and efficiency of different optimization strategies. Below is an overview of the synthetic problems included in this benchmark suite:</p>
<ul class="simple">
<li><p><strong>Sphere:</strong> A simple convex problem that is often used as a baseline. The global minimum is located at the origin, and the objective function value increases quadratically with distance from the origin.</p></li>
<li><p><strong>Rastrigin:</strong> A non-convex problem characterized by a large number of local minima, making it challenging for optimization algorithms to find the global minimum.</p></li>
<li><p><strong>Schwefel:</strong> Known for its complex landscape with many local minima, the Schwefel function requires optimization algorithms to balance exploration and exploitation effectively.</p></li>
<li><p><strong>Ackley:</strong> A multi-modal function with a nearly flat outer region and a large hole at the center, making it difficult for algorithms to escape local minima and converge to the global minimum.</p></li>
<li><p><strong>Levy:</strong> A multi-modal problem with a complex landscape that tests an algorithm’s ability to handle irregularities and identify global optima.</p></li>
<li><p><strong>Griewank:</strong> A function with many widespread local minima, making it challenging to converge to the global optimum. It is often used to assess the ability of algorithms to avoid getting trapped in local minima.</p></li>
<li><p><strong>Rosenbrock:</strong> A non-convex problem with a narrow, curved valley that contains the global minimum. This function is commonly used to test the convergence properties of optimization algorithms.</p></li>
<li><p><strong>Dropwave:</strong> A challenging multi-modal function with steep drops, requiring careful search strategies to avoid local minima.</p></li>
<li><p><strong>Langermann:</strong> This problem has many local minima and a highly irregular structure, testing an algorithm’s ability to explore complex search spaces.</p></li>
<li><p><strong>Rotated Hyper-Ellipsoid:</strong> A rotated version of the ellipsoid function, which tests an algorithm’s capability to optimize problems with rotated and ill-conditioned landscapes.</p></li>
<li><p><strong>Sum of Different Powers:</strong> A problem where each term in the sum contributes differently to the overall objective, requiring optimization algorithms to handle varying sensitivities across dimensions.</p></li>
<li><p><strong>Styblinski-Tang:</strong> A function with multiple global minima, commonly used to test an algorithm’s ability to avoid suboptimal solutions.</p></li>
<li><p><strong>Powell:</strong> A problem designed to challenge optimization algorithms with a mixture of convex and non-convex characteristics across different dimensions.</p></li>
<li><p><strong>Dixon-Price:</strong> This function has a smooth, narrow valley leading to the global minimum, testing an algorithm’s ability to navigate such features.</p></li>
<li><p><strong>Ellipsoid:</strong> A test problem that features high conditioning and elliptical level sets, requiring algorithms to efficiently search in skewed spaces.</p></li>
<li><p><strong>Discus:</strong> A variant of the sphere function with a large difference in scale between the first variable and the rest, making it a test of handling unbalanced scales.</p></li>
<li><p><strong>BentCigar:</strong> A highly anisotropic function where one direction has a much larger scale than the others, challenging algorithms to adjust their search strategies accordingly.</p></li>
<li><p><strong>SharpRidge:</strong> This function has a sharp ridge along one dimension, testing an algorithm’s ability to optimize in narrow, high-gradient regions.</p></li>
<li><p><strong>Katsuura:</strong> A multi-fractal function that combines periodicity and complexity, testing the capability of algorithms to explore intricate landscapes.</p></li>
<li><p><strong>Weierstrass:</strong> A problem with a fractal structure, characterized by a large number of local minima and requiring algorithms to handle varying scales of roughness.</p></li>
<li><p><strong>Different Powers:</strong> A problem where each term contributes differently to the objective, challenging algorithms to manage varying sensitivities and scales.</p></li>
<li><p><strong>Trid:</strong> A function that has a curved and ridge-like structure, often used to assess the convergence properties of optimization algorithms.</p></li>
<li><p><strong>LinearSlope:</strong> A simple linear function with a varying slope across dimensions, used to test the basic exploration capabilities of optimization methods.</p></li>
<li><p><strong>Elliptic:</strong> Similar to the Ellipsoid function but with exponentially increasing scales, testing an algorithm’s ability to search efficiently in poorly conditioned spaces.</p></li>
<li><p><strong>PERM:</strong> A complex combinatorial problem that combines different power terms, testing an algorithm’s ability to handle permutation-based search spaces.</p></li>
<li><p><strong>Power Sum:</strong> A problem where each dimension contributes a power sum to the objective, requiring algorithms to handle large variations in sensitivity across variables.</p></li>
<li><p><strong>Zakharov:</strong> A problem with a complex, non-linear interaction between variables, used to test an algorithm’s ability to navigate multi-variable coupling.</p></li>
<li><p><strong>Six-Hump Camel:</strong> A low-dimensional, multi-modal problem with several local minima, requiring precise search strategies to find the global optimum.</p></li>
<li><p><strong>Michalewicz:</strong> A problem known for its challenging steepness and periodicity, making it difficult for algorithms to locate the global minimum.</p></li>
<li><p><strong>Moving Peak:</strong> A dynamic optimization problem where the objective function changes over time, used to assess an algorithm’s adaptability to changing landscapes.</p></li>
</ul>
<p>These problems collectively provide a comprehensive suite for evaluating optimization algorithms across a broad range of difficulties, including convexity, multi-modality, separability, and conditioning.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Problem name</p></th>
<th class="head"><p>Mathematical formulation</p></th>
<th class="head"><p>Range</p></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Sphere</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d x_i^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5.12, 5.12]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Rastrigin</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = 10 d + \sum_{i=1}^d \left[ x_i^2 - 10 \cos(2 \pi x_i) \right]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-32.768, 32.768]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Schwefel</p></td>
<td colspan="2"><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = 418.9829 d - \sum_{i=1}^d x_i \sin\left(\sqrt{\left{x_i\right}\right)\)</span>                                                                          | <span class="math notranslate nohighlight">\(x_i \in [-500, 500]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Ackley</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = -a \exp \left(-b \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}\right)\)</span>
<span class="math notranslate nohighlight">\(-\exp \left(\frac{1}{d} \sum_{i=1}^d \cos \left(c x_i\right)\right) + a + \exp(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-32.768, 32.768]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Levy</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sin^2\left(\pi w_1\right) + \sum_{i=1}^{d-1}\left(w_i - 1\right)^2\)</span>
<span class="math notranslate nohighlight">\(\left[1 + 10 \sin^2\left(\pi w_i + 1\right)\right] + \left(w_d - 1\right)^2\)</span>
<span class="math notranslate nohighlight">\(\left[1 + \sin^2\left(2 \pi w_d\right)\right], w_i = 1 + \frac{x_i - 1}{4}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-10, 10]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Griewank</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d \frac{x_i^2}{4000} - \prod_{i=1}^d \cos\left(\frac{x_i}{\sqrt{i}}\right) + 1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-600, 600]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Rosenbrock</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^{d-1}\left[100\left(x_{i+1} - x_i^2\right)^2 + \left(x_i - 1\right)^2\right]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 10]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Dropwave</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = -\frac{1 + \cos\left(12 \sqrt{x_1^2 + x_2^2}\right)}{0.5\left(x_1^2 + x_2^2\right) + 2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5.12, 5.12]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Langermann</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^m c_i \exp\left(-\frac{1}{\pi} \sum_{j=1}^d \left(x_j - A_{ij}\right)^2\right)\)</span>
<span class="math notranslate nohighlight">\(\cos\left(\pi \sum_{j=1}^d\left(x_j - A_{ij}\right)^2\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [0, 10]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Rotated Hyper-Ellipsoid</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d \sum_{j=1}^i x_j^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-65.536, 65.536]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Sum of Different Powers</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d x_i^{i+1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-1, 1]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Styblinski-Tang</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \frac{1}{2} \sum_{i=1}^d\left(x_i^4 - 16 x_i^2 + 5 x_i\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Powell</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^{d/4}\left(x_{4i-3} + 10 x_{4i-2}\right)^2\)</span>
<span class="math notranslate nohighlight">\(+ 5\left(x_{4i-1} - x_{4i}\right)^2\)</span>
<span class="math notranslate nohighlight">\(+ \left(x_{4i-2} - 2 x_{4i-1}\right)^4\)</span>
<span class="math notranslate nohighlight">\(+ 10\left(x_{4i-3} - x_{4i}\right)^4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-4, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Dixon-Price</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \left(x_1 - 1\right)^2 + \sum_{i=2}^d i\left(2 x_i^2 - x_{i-1}\right)^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-10, 10]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Ellipsoid</p></td>
<td><p><span class="math notranslate nohighlight">\(f_2(\mathbf{x}) = \sum_{i=1}^D 10^{6 \frac{i-1}{D-1}} z_i^2 + f_{\mathrm{opt}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Discus</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = 10^6 x_1^2 + \sum_{i=2}^D x_i^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>BentCigar</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = x_1^2 + 10^6 \sum_{i=2}^n x_i^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>SharpRidge</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = x_1^2 + 100 \sqrt{\sum_{i=2}^D x_i^2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Katsuura</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \frac{10}{D^2} \prod_{i=1}^D \left(1 + i \sum_{j=1}^{32} \frac{2^j x_i - \left[2^j x_i\right]}{2^j}\right)^{10 / D^{1.2}}\)</span>
<span class="math notranslate nohighlight">\(- \frac{10}{D^2} + f_{\mathrm{pen}}(\mathbf{x})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Weierstrass</p></td>
<td><p><span class="math notranslate nohighlight">\(f_{16}(\mathbf{x}) = 10 \left(\frac{1}{D} \sum_{i=1}^D \sum_{k=0}^{11} \frac{1}{2^k} \cos \left(2 \pi 3^k\left(z_i + \frac{1}{2}\right)\right) - f_0\right)^3\)</span>
<span class="math notranslate nohighlight">\(+ \frac{10}{D} f_{\mathrm{pen}}(\mathbf{x})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DifferentPowers</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sqrt{\sum_{i=1}^D x_i^{2 + 4 \frac{i-1}{D-1}}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Trid</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d \left(x_i - 1\right)^2 - \sum_{i=2}^d x_i x_{i-1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-d^2, d^2]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>LinearSlope</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^D 5 s_i - s_i x_i\)</span>
<span class="math notranslate nohighlight">\(s_i = \operatorname{sign}\left(x_i^{\mathrm{opt}}\right) 10^{\frac{i-1}{D-1}},\)</span>
<span class="math notranslate nohighlight">\(\text{for } i=1, \ldots, D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Elliptic</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^D \left(10^6\right)^{\frac{i-1}{D-1}} x_i^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 5]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>PERM</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d \left(\sum_{j=1}^d \left(j + \beta\right)\left(x_j^i - \frac{1}{j^i}\right)\right)^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-d, d]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Power Sum</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d \left[\left(\sum_{j=1}^d x_j^i\right) - b_i\right]^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [0, d]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Zakharov</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d x_i^2 + \left(\sum_{i=1}^d 0.5 i x_i\right)^2\)</span>
<span class="math notranslate nohighlight">\(+ \left(\sum_{i=1}^d 0.5 i x_i\right)^4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-5, 10]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Six-Hump Camel</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \left(4 - 2.1 x_1^2 + \frac{x_1^4}{3}\right) x_1^2 + x_1 x_2\)</span>
<span class="math notranslate nohighlight">\(+ \left(-4 + 4 x_2^2\right) x_2^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_1 \in [-3, 3], x_2 \in [-2, 2]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Michalewicz</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = -\sum_{i=1}^d \sin \left(x_i\right) \sin ^{2 m}\left(\frac{i x_i^2}{\pi}\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [0, \pi]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Moving Peak</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^D \left(10^6\right)^{\frac{i-1}{D-1}} x_i^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [0, 100]\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>PERM 2</p></td>
<td><p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{i=1}^d\left(\sum_{j=1}^d\left(j^i+\beta\right)\left(\left(\frac{x_j}{j}\right)^i-1\right)\right)^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_i \in [-d, d]\)</span></p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="hyperparameter-optimization-problem">
<span id="hpo-problems"></span><h2>Hyperparameter Optimization Problem<a class="headerlink" href="#hyperparameter-optimization-problem" title="Link to this heading"></a></h2>
<p>This section provides an overview of the hyperparameter optimization problem including the hyperparameters used for various machine learning models and machine learning tasks used for generate problem instances.</p>
<p>Hyperparameters for Support Vector Machine (SVM)</p>
<p>Support Vector Machines (SVM) are widely used for classification and regression tasks. They are particularly effective in high-dimensional spaces and situations where the number of dimensions exceeds the number of samples. The hyperparameters for SVM control the regularization and the kernel function, which are crucial for model performance.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>C</p></td>
<td><p>[-10, 10]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>gamma</p></td>
<td><p>[-10, 10]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for AdaBoost</p>
<p>AdaBoost is a popular ensemble method that combines multiple weak learners to create a strong classifier. It is particularly useful for boosting the performance of decision trees. The hyperparameters control the number of estimators and the learning rate, which affects the contribution of each classifier.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>n_estimators</p></td>
<td><p>[1, 100]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-odd"><td><p>learning_rate</p></td>
<td><p>[0.01, 1]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for Random Forest</p>
<p>Random Forest is an ensemble learning method that builds multiple decision trees and merges them to get a more accurate and stable prediction. It is widely used for both classification and regression tasks. The hyperparameters include the number of trees, the depth of the trees, and various criteria for splitting nodes.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>n_estimators</p></td>
<td><p>[1, 1000]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-odd"><td><p>max_depth</p></td>
<td><p>[1, 100]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-even"><td><p>criterion</p></td>
<td><p>{gini, entropy}</p></td>
<td><p>Categorical</p></td>
</tr>
<tr class="row-odd"><td><p>min_samples_leaf</p></td>
<td><p>[1, 20]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-even"><td><p>min_weight_fraction_leaf</p></td>
<td><p>[0.0, 0.5]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>min_impurity_decrease</p></td>
<td><p>[0.0, 1.0]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for XGBoost</p>
<p>XGBoost is an efficient and scalable implementation of gradient boosting, designed for speed and performance. It is widely used in machine learning competitions and industry for classification and regression tasks. The hyperparameters include learning rates, tree depths, and regularization parameters, which control the complexity of the model and its ability to generalize.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>eta</p></td>
<td><p>[-10.0, 0.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>max_depth</p></td>
<td><p>[1, 15]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-even"><td><p>min_child_weight</p></td>
<td><p>[0.0, 7.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>colsample_bytree</p></td>
<td><p>[0.01, 1.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>colsample_bylevel</p></td>
<td><p>[0.01, 1.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>reg_lambda</p></td>
<td><p>[-10.0, 10.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>reg_alpha</p></td>
<td><p>[-10.0, 10.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>subsample_per_it</p></td>
<td><p>[0.1, 1.0]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>n_estimators</p></td>
<td><p>[1, 50]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-odd"><td><p>gamma</p></td>
<td><p>[0.0, 1.0]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for GLMNet</p>
<p>GLMNet is a regularized regression model that supports both LASSO and ridge regression. It is particularly useful for high-dimensional datasets where regularization is necessary to prevent overfitting. The hyperparameters control the strength of the regularization and the balance between L1 and L2 penalties.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>lambda</p></td>
<td><p>[0, 10^5]</p></td>
<td><p>Log-integer</p></td>
</tr>
<tr class="row-odd"><td><p>alpha</p></td>
<td><p>[0, 1]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>nlambda</p></td>
<td><p>[1, 100]</p></td>
<td><p>Integer</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for AlexNet</p>
<p>AlexNet is a convolutional neural network (CNN) architecture that revolutionized the field of computer vision by achieving significant improvements on the ImageNet dataset. The hyperparameters include learning rate, dropout rate, weight decay, and the choice of activation function, all of which are crucial for training deep neural networks.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning_rate</p></td>
<td><p>[10^-5, 10^-1]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>dropout_rate</p></td>
<td><p>[0.0, 0.5]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>weight_decay</p></td>
<td><p>[10^-5, 10^-2]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>activation_function</p></td>
<td><p>{ReLU, Leaky ReLU, ELU}</p></td>
<td><p>Categorical</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for 2-Layer Bayesian Neural Network (BNN)</p>
<p>Bayesian Neural Networks (BNNs) provide a probabilistic interpretation of deep learning models by introducing uncertainty in the weights. This allows BNNs to express model uncertainty, which is crucial for tasks where uncertainty quantification is important. The hyperparameters include layer sizes, step length, burn-in period, and momentum decay.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>layer 1</p></td>
<td><p>[2^4, 2^9]</p></td>
<td><p>Log-integer</p></td>
</tr>
<tr class="row-odd"><td><p>layer 2</p></td>
<td><p>[2^4, 2^9]</p></td>
<td><p>Log-integer</p></td>
</tr>
<tr class="row-even"><td><p>step_length</p></td>
<td><p>[10^-6, 10^-1]</p></td>
<td><p>Log-continuous</p></td>
</tr>
<tr class="row-odd"><td><p>burn_in</p></td>
<td><p>[0, 8]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-even"><td><p>momentum_decay</p></td>
<td><p>[0, 1]</p></td>
<td><p>Log-continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for CNNs</p>
<p>Convolutional Neural Networks (CNNs) are the backbone of most modern computer vision systems. They are designed to automatically and adaptively learn spatial hierarchies of features through backpropagation. The hyperparameters include learning rate, momentum, regularization parameter, dropout rate, and activation function.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning_rate</p></td>
<td><p>[10^-6, 10^-1]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>momentum</p></td>
<td><p>[0.0, 0.9]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>regularization_parameter</p></td>
<td><p>[10^-6, 10^-2]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>dropout_rate</p></td>
<td><p>[0, 0.5]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>activation_function</p></td>
<td><p>{ReLU, Leaky ReLU, Tanh, Sigmoid}</p></td>
<td><p>Categorical</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for ResNet18</p>
<p>ResNet18 is a residual network architecture that introduced the concept of residual connections, allowing for the training of very deep networks by mitigating the vanishing gradient problem. The hyperparameters include learning rate, momentum, dropout rate, and weight decay.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning_rate</p></td>
<td><p>[2^3, 2^8]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-odd"><td><p>momentum</p></td>
<td><p>[0, 1]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>dropout_rate</p></td>
<td><p>[0, 0.5]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>weight_decay</p></td>
<td><p>[10^-5, 10^-1]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Hyperparameters for DenseNet</p>
<p>DenseNet is a densely connected convolutional network that connects each layer to every other layer in a feed-forward fashion. This architecture improves the flow of information and gradients throughout the network, making it easier to train. The hyperparameters include learning rate, momentum, dropout rate, and weight decay.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>learning_rate</p></td>
<td><p>[2^3, 2^8]</p></td>
<td><p>Integer</p></td>
</tr>
<tr class="row-odd"><td><p>momentum</p></td>
<td><p>[0, 1]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-even"><td><p>dropout_rate</p></td>
<td><p>[0, 0.5]</p></td>
<td><p>Continuous</p></td>
</tr>
<tr class="row-odd"><td><p>weight_decay</p></td>
<td><p>[10^-5, 10^-1]</p></td>
<td><p>Continuous</p></td>
</tr>
</tbody>
</table>
<p>Machine Learning Tasks</p>
<p>This section lists the various datasets used for machine learning tasks, including classification and regression problems. These datasets are widely recognized in the machine learning community and are used for benchmarking algorithms.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Source</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
<th class="head"><p><strong>Number</strong></p></th>
<th class="head"><p><strong>IDs</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[OpenML-CC18](<a class="reference external" href="https://www.openml.org/s/99">https://www.openml.org/s/99</a>)</p></td>
<td><p>Classification</p></td>
<td><p>78</p></td>
<td><p>1-78</p></td>
</tr>
<tr class="row-odd"><td><p>[UC Irvine Repository](<a class="reference external" href="https://archive.ics.uci.edu/">https://archive.ics.uci.edu/</a>)</p></td>
<td><p>Classification/Regression</p></td>
<td><p>10</p></td>
<td><p>79-88</p></td>
</tr>
<tr class="row-even"><td><p>[NAS-Bench-360](<a class="reference external" href="https://archive.ics.uci.edu/">https://archive.ics.uci.edu/</a>)</p></td>
<td><p>Classification/Regression</p></td>
<td><p>5</p></td>
<td><p>89-93</p></td>
</tr>
<tr class="row-odd"><td><p>[NATS-Bench](<a class="reference external" href="https://github.com/D-X-Y/NATS-Bench">https://github.com/D-X-Y/NATS-Bench</a>)</p></td>
<td><p>Classification</p></td>
<td><p>3</p></td>
<td><p>94-96</p></td>
</tr>
<tr class="row-even"><td><p>[SVHN](<a class="reference external" href="https://github.com/D-X-Y/NATS-Bench">https://github.com/D-X-Y/NATS-Bench</a>)</p></td>
<td><p>Classification</p></td>
<td><p>1</p></td>
<td><p>97</p></td>
</tr>
</tbody>
</table>
</section>
<section id="configurable-software-optimization-problem">
<span id="cso-problems"></span><h2>Configurable Software Optimization Problem<a class="headerlink" href="#configurable-software-optimization-problem" title="Link to this heading"></a></h2>
<p>This section provides a summary of the configurable software optimization (CSO) tasks, which involve optimizing various software systems. The tasks are characterized by the number of variables, objectives, and workloads, along with the sources of these workloads.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Software Name</strong></p></th>
<th class="head"><p><strong>Variables</strong></p></th>
<th class="head"><p><strong>Objectives</strong></p></th>
<th class="head"><p><strong>Workloads</strong></p></th>
<th class="head"><p><strong>Workloads Source</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LLVM</p></td>
<td><p>93</p></td>
<td><p>8</p></td>
<td><p>50</p></td>
<td><p>[PolyBench](<a class="reference external" href="https://web.cs.ucla.edu/~pouchet/software/polybench/">https://web.cs.ucla.edu/~pouchet/software/polybench/</a>), [mibench](<a class="reference external" href="https://github.com/embecosm/mibench?tab=readme-ov-file">https://github.com/embecosm/mibench?tab=readme-ov-file</a>)</p></td>
</tr>
<tr class="row-odd"><td><p>GCC</p></td>
<td><p>105</p></td>
<td><p>8</p></td>
<td><p>50</p></td>
<td><p>[PolyBench](<a class="reference external" href="https://web.cs.ucla.edu/~pouchet/software/polybench/">https://web.cs.ucla.edu/~pouchet/software/polybench/</a>), [mibench](<a class="reference external" href="https://github.com/embecosm/mibench?tab=readme-ov-file">https://github.com/embecosm/mibench?tab=readme-ov-file</a>)</p></td>
</tr>
<tr class="row-even"><td><p>Mysql</p></td>
<td><p>28</p></td>
<td><p>14</p></td>
<td><p>18</p></td>
<td><p>[benchbase](<a class="reference external" href="https://github.com/cmu-db/benchbase.git">https://github.com/cmu-db/benchbase.git</a>), [sysbench](<a class="reference external" href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a>)</p></td>
</tr>
<tr class="row-odd"><td><p>Hadoop</p></td>
<td><p>206</p></td>
<td><p>1</p></td>
<td><p>29</p></td>
<td><p>[HiBench](<a class="reference external" href="https://github.com/Intel-bigdata/HiBench">https://github.com/Intel-bigdata/HiBench</a>)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="rna-inverse-design-problem">
<span id="rna-problems"></span><h2>RNA Inverse Design Problem<a class="headerlink" href="#rna-inverse-design-problem" title="Link to this heading"></a></h2>
<p>RNA inverse design involves designing RNA sequences that fold into specific secondary structures. This task is crucial for understanding and manipulating RNA function in various biological processes. The datasets listed here are commonly used benchmarks for RNA design algorithms.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Source</strong></p></th>
<th class="head"><p><strong>Min-Max Length (nt)</strong></p></th>
<th class="head"><p><strong>Samples</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[Eterna100](<a class="reference external" href="https://github.com/eternagame/eterna100-benchmarking">https://github.com/eternagame/eterna100-benchmarking</a>)</p></td>
<td><p>11-399</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>[Rfam-learn test](<a class="reference external" href="https://rfam.org/">https://rfam.org/</a>)</p></td>
<td><p>50-446</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-even"><td><p>[RNA-Strand](<a class="reference external" href="http://www.rnasoft.ca/strand/">http://www.rnasoft.ca/strand/</a>)</p></td>
<td><p>4-4381</p></td>
<td><p>50</p></td>
</tr>
<tr class="row-odd"><td><p>[RNAStralign](<a class="reference external" href="https://github.com/D-X-Y/NATS-Bench">https://github.com/D-X-Y/NATS-Bench</a>)</p></td>
<td><p>30-1851</p></td>
<td><p>37149</p></td>
</tr>
<tr class="row-even"><td><p>[ArchiveII](<a class="reference external" href="https://github.com/D-X-Y/NATS-Bench">https://github.com/D-X-Y/NATS-Bench</a>)</p></td>
<td><p>28-2968</p></td>
<td><p>2975</p></td>
</tr>
</tbody>
</table>
</section>
<section id="protein-inverse-folding-problem">
<span id="pif-problems"></span><h2>Protein Inverse Folding Problem<a class="headerlink" href="#protein-inverse-folding-problem" title="Link to this heading"></a></h2>
<p>Protein Inverse Folding involves creating new amino acids sequence folding into desiered backbone structure. These problems are essential for applications in drug design, biotechnology, and synthetic biology. The datasets listed here are widely used in protein inverse folding research.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Source</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
<th class="head"><p><strong>Numbers</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[Absolute](<a class="reference external" href="https://github.com/csi-greifflab/Absolut">https://github.com/csi-greifflab/Absolut</a>)</p></td>
<td><p>Antibody design</p></td>
<td><p>159</p></td>
</tr>
<tr class="row-odd"><td><p>[CATH](<a class="reference external" href="https://www.cathdb.info/">https://www.cathdb.info/</a>)</p></td>
<td><p>Single-chain protein design</p></td>
<td><p>19752</p></td>
</tr>
<tr class="row-even"><td><p>[Protein Data Bank](<a class="reference external" href="https://www.rcsb.org/">https://www.rcsb.org/</a>)</p></td>
<td><p>Multi-chain protein design</p></td>
<td><p>26361</p></td>
</tr>
</tbody>
</table>
</section>
<section id="parallelization">
<span id="id1"></span><h2>Parallelization<a class="headerlink" href="#parallelization" title="Link to this heading"></a></h2>
<p>To-do</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="algorithms.html" class="btn btn-neutral float-left" title="Algorithmic objects" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="results.html" class="btn btn-neutral float-right" title="Results Analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Peili Mao.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>