<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Results Analysis &mdash; TransOpt 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=01f34227"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="visualization" href="visualization.html" />
    <link rel="prev" title="Benchmark Problems" href="problems.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TransOpt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithmic objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="problems.html">Benchmark Problems</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Results Analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#register-a-new-results-analysis-method">Register a New Results Analysis Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="#customization-analysis-pipline">Customization Analysis Pipline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#list-of-performance-evaluation-metrics">List of Performance Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#statistical-measures">Statistical Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wilcoxon-signed-rank-test">Wilcoxon Signed-Rank Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scott-knott-test">Scott-Knott Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a12-effect-size">A12 Effect Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="#critical-difference-cd">Critical Difference (CD)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Using TransOpt via Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/architecture.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TransOpt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Results Analysis</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/usage/results.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="results-analysis">
<h1>Results Analysis<a class="headerlink" href="#results-analysis" title="Link to this heading"></a></h1>
<div class="info admonition">
<p class="admonition-title">Overview</p>
<ul class="simple">
<li><p><a class="reference internal" href="#registering-new-analysis"><span class="std std-ref">Register a New Results Analysis Method</span></a>: How to add a new results analysis method to <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a>.</p></li>
<li><p><a class="reference internal" href="#customization"><span class="std std-ref">Customization Analysis Pipline</span></a>: How to customize your own results analysis pipline or add your own analysis method into the pipline.</p></li>
<li><p><a class="reference internal" href="#performance-evaluation-metrics"><span class="std std-ref">Performance Evaluation Metrics</span></a>: The list of the performance evaluation metrics available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
<li><p><a class="reference internal" href="#statistical-measures"><span class="std std-ref">Statistical Measures</span></a>: The list of the statistical measures supportede in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOpt</span></a></p></li>
</ul>
</div>
<section id="register-a-new-results-analysis-method">
<span id="registering-new-analysis"></span><h2>Register a New Results Analysis Method<a class="headerlink" href="#register-a-new-results-analysis-method" title="Link to this heading"></a></h2>
</section>
<section id="customization-analysis-pipline">
<span id="customization"></span><h2>Customization Analysis Pipline<a class="headerlink" href="#customization-analysis-pipline" title="Link to this heading"></a></h2>
</section>
<section id="list-of-performance-evaluation-metrics">
<span id="performance-evaluation-metrics"></span><h2>List of Performance Evaluation Metrics<a class="headerlink" href="#list-of-performance-evaluation-metrics" title="Link to this heading"></a></h2>
<p>For each type of task instance, the framework offers performance evaluation metrics to assess the quality of the solutions generated by the algorithms. The metrics are categorized based on the type of task and are designed to evaluate various aspects of the solutions. The tables below summarize the performance metrics available for different tasks.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Task</strong></p></th>
<th class="head"><p><strong>Metric</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
<th class="head"><p><strong>Scale</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Synthetic</strong></p></td>
<td><p>Absolute Error</p></td>
<td><p>The difference between the min value and the optimal
solution.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>HPO (Classification)</strong></p></td>
<td><p>F1 Score</p></td>
<td><p>The mean of precision and recall, providing a balanced
measure of accuracy.</p></td>
<td><p>[0, 1]</p></td>
<td><p>Maximization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>Area Under Curve</p></td>
<td><p>The area under the receiver operating characteristic
(ROC) curve, quantifying the overall ability of a classifier
to discriminate between positive and negative instances.</p></td>
<td><p>[0, 1]</p></td>
<td><p>Maximization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>HPO (Regression)</strong></p></td>
<td><p>RMSE</p></td>
<td><p>Root mean squared error (RMSE) measures the average
magnitude of the differences between predicted values and
actual values.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>MAE</p></td>
<td><p>Mean absolute error (MAE) measures the average absolute
differences between predicted values and actual values.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Protein Design</strong></p></td>
<td><p>Binding Affinity</p></td>
<td><p>The strength of the interaction between a protein and its
ligand, typically measured by the equilibrium dissociation
constant.</p></td>
<td><p>[-∞, 0]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td><p><strong>RNA Inverse Design</strong></p></td>
<td><p>GC-content</p></td>
<td><p>The percentage of guanine (G) and cytosine (C) bases in a
DNA or RNA molecule, which affects the stability and
melting temperature.</p></td>
<td><p>[0, 1]</p></td>
<td><p>Maximization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LLVM/GCC</strong></p></td>
<td><p>Avg Execution Time</p></td>
<td><p>The average execution time of multiple runs.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>Compilation Time</p></td>
<td><p>The time required to compile the code.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>File Size</p></td>
<td><p>The size of the executable file generated after compilation.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>Max RSS</p></td>
<td><p>The maximum resident set size used during execution.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>PAPI TOT CYC</p></td>
<td><p>The total number of CPU cycles consumed during execution.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>PAPI TOT INS</p></td>
<td><p>The total number of instructions executed by the CPU.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>PAPI BR MSP</p></td>
<td><p>The number of times the CPU mispredicted branch directions.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>PAPI BR PRC</p></td>
<td><p>The number of times the CPU correctly predicted branch
directions.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>PAPI BR CN</p></td>
<td><p>The number of conditional branch instructions.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>PAPI MEM WCY</p></td>
<td><p>The number of cycles spent waiting for memory access.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MySQL</strong></p></td>
<td><p>Throughput</p></td>
<td><p>The number of transactions processed per unit of time.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Maximization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>Latency</p></td>
<td><p>The time required to complete a single transaction from
initiation to completion.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>CPU Usage</p></td>
<td><p>The proportion of CPU resources used during database
operations.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>Memory Usage</p></td>
<td><p>The amount of memory resources used during database
operations.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Hadoop</strong></p></td>
<td><p>Execution Time</p></td>
<td><p>The execution time of a big data task.</p></td>
<td><p>[0, ∞]</p></td>
<td><p>Minimization</p></td>
</tr>
</tbody>
</table>
</section>
<section id="statistical-measures">
<span id="id1"></span><h2>Statistical Measures<a class="headerlink" href="#statistical-measures" title="Link to this heading"></a></h2>
<p>This section provides detailed explanations of the statistical methods used for analyzing the performance of different algorithms. Each method is accompanied by the relevant formulas and calculation procedures.</p>
</section>
<section id="wilcoxon-signed-rank-test">
<h2>Wilcoxon Signed-Rank Test<a class="headerlink" href="#wilcoxon-signed-rank-test" title="Link to this heading"></a></h2>
<p>The <strong>Wilcoxon signed-rank test</strong> is a non-parametric statistical test used to compare two paired samples. Unlike the paired t-test, the Wilcoxon signed-rank test does not assume that the differences between pairs are normally distributed. It is particularly useful when dealing with small sample sizes or non-normally distributed data.</p>
<p>Given two related samples <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, the steps to perform the Wilcoxon signed-rank test are:</p>
<ol class="arabic">
<li><p><strong>Compute the differences</strong> between each pair of observations: <span class="math notranslate nohighlight">\(d_i = X_i - Y_i\)</span>.</p></li>
<li><p><strong>Rank the absolute values</strong> of the differences, assigning ranks from the smallest to the largest difference.</p></li>
<li><p><strong>Assign signs</strong> to the ranks based on the sign of the original differences <span class="math notranslate nohighlight">\(d_i\)</span>.</p></li>
<li><p><strong>Calculate the test statistic</strong> <span class="math notranslate nohighlight">\(W\)</span>, which is the sum of the ranks corresponding to the positive differences:</p>
<div class="math notranslate nohighlight">
\[W = \sum_{d_i &gt; 0} \text{Rank}(d_i)\]</div>
</li>
<li><p>Compare the computed test statistic <span class="math notranslate nohighlight">\(W\)</span> against the critical value from the Wilcoxon signed-rank table or calculate the p-value to determine the significance of the result.</p></li>
</ol>
</section>
<section id="scott-knott-test">
<h2>Scott-Knott Test<a class="headerlink" href="#scott-knott-test" title="Link to this heading"></a></h2>
<p>The <strong>Scott-Knott test</strong> is a statistical method used to rank the performance of different techniques across multiple runs on each benchmark instance. It is particularly effective in scenarios where multiple comparisons are being made, and it controls the family-wise error rate.</p>
<p>The procedure involves:</p>
<ol class="arabic simple">
<li><p><strong>Partitioning the data</strong>: Initially, all techniques are considered in one group. The group is then split into two subgroups if the mean difference between them is statistically significant.</p></li>
<li><p><strong>Calculating the mean difference</strong> between the groups using an appropriate test (e.g., ANOVA or t-test).</p></li>
<li><p><strong>Assigning ranks</strong>: If a significant difference is found, the techniques are ranked within their respective subgroups. If no significant difference is found, the techniques are considered to be in the same rank.</p></li>
<li><p><strong>Repeating the process</strong> until no further significant splits can be made.</p></li>
</ol>
<p>The Scott-Knott test is particularly useful for determining the relative performance of multiple techniques, providing a clear ranking based on statistically significant differences.</p>
</section>
<section id="a12-effect-size">
<h2>A12 Effect Size<a class="headerlink" href="#a12-effect-size" title="Link to this heading"></a></h2>
<p>The <strong>A12 effect size</strong> is a non-parametric measure used to evaluate the probability that one algorithm outperforms another. It is particularly useful in understanding whether observed differences are practically significant, beyond just being statistically significant.</p>
<p>The A12 statistic is calculated as follows:</p>
<ol class="arabic">
<li><p>Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be the two sets of performance measures for two algorithms.</p></li>
<li><p><strong>Calculate the A12 statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[A_{12} = \frac{\sum_{x \in A} \sum_{y \in B} \mathbf{I}(x &gt; y) + 0.5 \cdot \mathbf{I}(x = y)}{|A| \cdot |B|}\]</div>
</li>
</ol>
</section>
<section id="critical-difference-cd">
<h2>Critical Difference (CD)<a class="headerlink" href="#critical-difference-cd" title="Link to this heading"></a></h2>
<p>The <strong>Critical Difference (CD)</strong> is a statistical measure used to assess whether performance differences between algorithms are derived from randomness. It is typically used in conjunction with methods like the Friedman test or Nemenyi post-hoc test to evaluate multiple algorithms across multiple datasets.</p>
<p>The steps involved in calculating the Critical Difference are:</p>
<ol class="arabic">
<li><p><strong>Perform a Friedman test</strong> to rank the algorithms for each dataset.</p></li>
<li><p><strong>Calculate the average ranks</strong> for each algorithm across all datasets.</p></li>
<li><p><strong>Compute the Critical Difference (CD)</strong> using the following formula:</p>
<div class="math notranslate nohighlight">
\[\text{CD} = q_{\alpha} \sqrt{\frac{k(k+1)}{6N}}\]</div>
<p>where:
- <span class="math notranslate nohighlight">\(q_{\alpha}\)</span> is the critical value for a given significance level <span class="math notranslate nohighlight">\(\alpha\)</span> from the studentized range statistic.
- <span class="math notranslate nohighlight">\(k\)</span> is the number of algorithms.
- <span class="math notranslate nohighlight">\(N\)</span> is the number of datasets.</p>
</li>
<li><p>If the difference in average ranks between two algorithms exceeds the CD, the performance difference is considered statistically significant, and not due to random variation.</p></li>
</ol>
<p>These statistical methods provide robust tools for comparing algorithm performance across various benchmarks, ensuring that conclusions drawn are both statistically and practically significant.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="problems.html" class="btn btn-neutral float-left" title="Benchmark Problems" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="visualization.html" class="btn btn-neutral float-right" title="visualization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Peili Mao.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>