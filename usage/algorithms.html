
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Algorithmic objects &#8212; TransOPT: Transfer Optimization System for Bayesian Optimization Using Transfer Learning 0.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=c0cc07d6" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=a1637f0b"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'usage/algorithms';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmark Problems" href="problems.html" />
    <link rel="prev" title="Quick Start" href="../quickstart.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/transopt_logo.jpg" class="logo__image only-light" alt="TransOPT: Transfer Optimization System for Bayesian Optimization Using Transfer Learning 0.1.0 documentation - Home"/>
    <script>document.write(`<img src="../_static/transopt_logo.jpg" class="logo__image only-dark" alt="TransOPT: Transfer Optimization System for Bayesian Optimization Using Transfer Learning 0.1.0 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Algorithmic objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="problems.html">Benchmark Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_manage.html">Data Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/architecture.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development/api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/usage/algorithms.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Algorithmic objects</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#registering-a-new-algorithm-in-transopt">Registering a New Algorithm in TransOPT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-algorithms">Supported Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#search-space-transform">Search space transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-design">Initialization Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogate-model">Surrogate Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acquisition-function">Acquisition Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#list-of-algorithmic-objects">List of Algorithmic Objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="algorithmic-objects">
<h1>Algorithmic objects<a class="headerlink" href="#algorithmic-objects" title="Link to this heading">#</a></h1>
<div class="info admonition">
<p class="admonition-title">Overview</p>
<ul class="simple">
<li><p><a class="reference internal" href="#register-new-algorithm"><span class="std std-ref">Register</span></a>: How to register a new algorithmic Object to <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOPT</span></a></p></li>
<li><p><a class="reference internal" href="#alg"><span class="std std-ref">Supported Algorithms</span></a>: The list of the synthetic problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOPT</span></a></p></li>
<li><p><a class="reference internal" href="#alg-obj"><span class="std std-ref">Algorithmic Objects</span></a>: The list of the protein inverse folding problems available in <a class="reference internal" href="../index.html#home"><span class="std std-ref">TransOPT</span></a></p></li>
</ul>
</div>
<section id="registering-a-new-algorithm-in-transopt">
<span id="register-new-algorithm"></span><h2>Registering a New Algorithm in TransOPT<a class="headerlink" href="#registering-a-new-algorithm-in-transopt" title="Link to this heading">#</a></h2>
<p>To register a new algorithm object in TransOPT, follow the steps outlined below:</p>
<ol class="arabic">
<li><p><strong>Import the Model Registry</strong></p>
<p>First, you need to import the <cite>model_registry</cite> from the <cite>transopt.agent.registry</cite> module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transopt.agent.registry</span> <span class="kn">import</span> <span class="n">model_registry</span>
</pre></div>
</div>
</li>
<li><p><strong>Define the Algorithm Object Name</strong></p>
<p>Next, use the registry to define the name of your algorithm object. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@model_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;MHGP&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MHGP</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>In this example, the algorithm object is named “MHGP”.</p>
</li>
<li><p><strong>Choose the Appropriate Base Class</strong></p>
<p>Depending on the type of algorithm object you are creating, you must inherit from a specific base class. TransOPT provides several algorithm modules, each corresponding to a different base class:</p>
<ul class="simple">
<li><p><strong>Surrogate Model</strong>: Inherit from the <cite>Model</cite> class.</p></li>
<li><p><strong>Initialization Design</strong>: Inherit from the <cite>Sampler</cite> class.</p></li>
<li><p><strong>Acquisition Function</strong>: Inherit from the <cite>AcquisitionBase</cite> class.</p></li>
<li><p><strong>Pretrain Module</strong>: Inherit from the <cite>PretrainBase</cite> class.</p></li>
<li><p><strong>Normalizer Module</strong>: Inherit from the <cite>NormalizerBase</cite> class.</p></li>
</ul>
<p>For instance, in the example provided, we are creating a surrogate model, so the <cite>MHGP</cite> class inherits from the <cite>Model</cite> base class.</p>
</li>
<li><p><strong>Implement the Required Abstract Methods</strong></p>
<p>Once the class is defined, you need to implement several abstract methods that are required by the <cite>Model</cite> base class. These methods include:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">meta_fit</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_X</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">source_Y</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">optimize</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">optimize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">return_full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">with_noise</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>meta_fit</strong>: This method is used to fit meta-data. If your transfer optimization algorithm requires meta-data, this is where you should leverage it.</p></li>
<li><p><strong>fit</strong>: This method is used to fit the data for the current task.</p></li>
</ul>
</li>
</ol>
<p>By following these steps, you can successfully register a new algorithm object in TransOPT and implement the necessary functionality to integrate it into the framework.</p>
</section>
<section id="supported-algorithms">
<span id="alg"></span><h2>Supported Algorithms<a class="headerlink" href="#supported-algorithms" title="Link to this heading">#</a></h2>
<section id="search-space-transform">
<h3>Search space transform<a class="headerlink" href="#search-space-transform" title="Link to this heading">#</a></h3>
<p><strong>Hyperparameter Search Space Pruning – A New Component for Sequential Model-Based Hyperparameter Optimization</strong><span id="id1">[<a class="reference internal" href="#id167" title="Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Hyperparameter search space pruning - a new component for sequential model-based hyperparameter optimization. In ECML/PKDD'15: Proc of the 2015 Advances in Machine Learning and Knowledge Discovery in Databases, volume 9285, 104–119. 2015.">12</a>]</span></p>
<p>This method prunes ineffective regions of the hyperparameter search space by using past evaluations to guide the optimization. It identifies areas with low potential by analyzing the performance of sampled configurations and employing a surrogate model to predict future outcomes. Regions that consistently show poor performance or low expected improvement are marked as low potential. The method then updates the search process to focus on more promising regions, thereby improving optimization efficiency and reducing unnecessary evaluations.</p>
<p><strong>Learning search spaces for Bayesian optimization- Another view of hyperparameter transfer learning</strong><span id="id2">[<a class="reference internal" href="#id168" title="Valerio Perrone and Huibin Shen. Learning search spaces for Bayesian optimization: another view of hyperparameter transfer learning. In NIPS'19: Proc of the 2019 Advances in Neural Information Processing Systems, 12751–12761. 2019.">8</a>]</span></p>
<p>The method replaces predefined search space with data-driven geometrical representations (e.g., ellipsoids and boxes) by analyzing historical data to identify high-performing regions and fitting these regions with geometrical shapes. This transformation narrows the search to promising areas, improving efficiency as the search space dimension increases.</p>
</section>
<section id="initialization-design">
<h3>Initialization Design<a class="headerlink" href="#initialization-design" title="Link to this heading">#</a></h3>
<p><strong>FEW-SHOT BAYESIAN OPTIMIZATION WITH DEEP KERNEL SURROGATES</strong><span id="id3">[<a class="reference internal" href="#id70" title="Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In ICLR'21: Proc. of the 9th International Conference on Learning Representations. OpenReview.net, 2021.">11</a>]</span></p>
<p>This method leverages historical task data and an evolutionary algorithm to provide a warm-start initialization. By selecting hyperparameter settings that minimize a loss function across multiple tasks, the method accelerates optimization with fewer evaluations.</p>
<p><strong>Initializing Bayesian Hyperparameter Optimization via Meta-Learning</strong><span id="id4">[<a class="reference internal" href="#id66" title="Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. In AAAI'15: Proc. of the 2015 AAAI Conference on Artificial Intelligence, 1128–1135. AAAI Press, 2015.">3</a>]</span></p>
<p>This method introduces a meta-learning-based initialization for BO, improving the starting point by leveraging hyperparameter configurations that worked well on similar datasets. These similar datasets are identified through meta-features. The method calculates the distance between datasets using these meta-features, selecting the most similar ones to initialize the optimization process efficiently.</p>
<p><strong>Learning Hyperparameter Optimization Initializations</strong><span id="id5">[<a class="reference internal" href="#id67" title="Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization initializations. In DSAA'15: Proc. of the 2015 IEEE International Conference on Data Science and Advanced Analytics, 1–10. IEEE, 2015.">13</a>]</span></p>
<p>This method proposes to use a meta-loss function that is minimized through gradient-based optimization. By optimizing for a meta-loss derived from the response functions of past datasets, it generates entirely new configurations, whereas prior methods limited themselves to reusing configurations in similar datasets.</p>
</section>
<section id="surrogate-model">
<h3>Surrogate Model<a class="headerlink" href="#surrogate-model" title="Link to this heading">#</a></h3>
<p><strong>Pre-trained Gaussian processes for Bayesian optimization</strong><span id="id6">[<a class="reference internal" href="#id98" title="Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained Gaussian processes for Bayesian optimization. arXiv preprint arXiv:2109.08215, 2021.">10</a>]</span></p>
<p>In this method, the surrogate model is built on a pre-trained GP with data from related tasks. This approach uses a KL divergence-based loss function to pre-train the GP, ensuring it captures similarities between the target function and past data. The pre-trained GP serves as the prior for BO, allowing the model to make better predictions with fewer observations by leveraging the pre-trained knowledge.</p>
<p><strong>FEW-SHOT BAYESIAN OPTIMIZATION WITH DEEP KERNEL SURROGATES</strong></p>
<p>In this method, the surrogate model is a deep kernel Gaussian process that is meta-learned across multiple past tasks. This model enables quick adaptation to new tasks with limited evaluations. The deep kernel, which combines a neural network and a Gaussian process, provides uncertainty estimates, helping the model generalize across diverse tasks while being fine-tuned for new ones.</p>
<p><strong>Google Vizier- A Service for Black-Box Optimization</strong><span id="id7">[<a class="reference internal" href="#id88" title="Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. Google vizier: A service for black-box optimization. In KDD'17: Proc. of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1487–1495. ACM, 2017.">4</a>]</span></p>
<p>This method transfers source knowledge by using the posterior mean of the source task as the prior mean for the target task. This approach simplifies the transfer process by ignoring uncertainty from the source model and only leveraging the mean, which leads to reduced computational complexity while still incorporating valuable information from the source task.</p>
<p><strong>PFNs4BO- In-Context Learning for Bayesian Optimization</strong><span id="id8">[<a class="reference internal" href="#id171" title="Samuel Müller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: in-context learning for Bayesian optimization. In ICML'23: Proc of the International Conference on Machine Learning, volume 202, 25444–25470. PMLR, 2023.">7</a>]</span></p>
<p>This method utilizes a Transformer-based architecture called Prior-data Fitted Networks (PFNs). These networks are trained on synthetic datasets to approximate the posterior predictive distribution (PPD) through in-context learning. PFNs can be trained on any efficiently sampled prior distribution, such as Gaussian processes or Bayesian neural networks. By learning from diverse priors, the PFN surrogate model captures complex patterns in the optimization process, allowing it to make accurate predictions while maintaining flexibility to incorporate user-defined priors or handle spurious dimensions effectively.</p>
<p><strong>Scalable Gaussian process-based transfer surrogates for hyperparameter optimization</strong><span id="id9">[<a class="reference internal" href="#id97" title="Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable Gaussian process-based transfer surrogates for hyperparameter optimization. Mach. Learn., 107(1):43–78, 2018.">14</a>]</span></p>
<p>This method introduces an ensemble of GP, where each GP is trained on a different past task. The model uses a weighted sum approach to combine the predictions from each GP. The weights are assigned based on how well each GP predicts the target task, with more relevant models receiving higher weights.</p>
<p><strong>Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles</strong><span id="id10">[<a class="reference internal" href="#id142" title="Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for Bayesian optimization using ranking-weighted Gaussian process ensembles. In ICML 2018 AutoML Workshop, volume 7, 1–15. 2018.">2</a>]</span></p>
<p>This method introduces Ranking-Weighted Gaussian Process Ensembles (RGPE). Similar to previous approaches, the surrogate model combines an ensemble of GPs. However, in RGPE, the weights are determined using a ranking loss function, which assesses how effectively each GP ranks the observations from the current task. GPs that rank the observations more accurately are assigned higher weights, reflecting their greater relevance to the task at hand.</p>
<p><strong>Multi-Task Bayesian Optimization</strong><span id="id11">[<a class="reference internal" href="#id60" title="Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task Bayesian optimization. In NIPs'13: Proc of the 2013 Annual Conference on Neural Information Processing Systems, 2004–2012. 2013.">9</a>]</span></p>
<p>This method uses multi-task Gaussian processes (MTGP) as the surrogate model. It trains a GP for each task and uses a shared covariance structure across tasks to improve predictive accuracy. By leveraging the relationships between tasks, the MTGP reduces the need for independent function evaluations, making the optimization process faster and more efficient.</p>
<p><strong>Multi-Fidelity Bayesian Optimization via Deep Neural Networks</strong><span id="id12">[<a class="reference internal" href="#id29" title="Shibo Li, Wei W. Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In NIPS'20: The Proc. of the 33th Advances in Neural Information Processing Systems. 2020.">6</a>]</span></p>
<p>In this method, the surrogate model employs a deep neural network designed to handle multi-fidelity optimization tasks. The DNN surrogate models each fidelity with a neural network, and higher fidelities are conditioned on the outputs from lower fidelities. By stacking neural networks for each fidelity level, the model captures nonlinear relationships between different fidelities. This structure allows the surrogate to propagate information across fidelities, improving the accuracy of function estimation at higher fidelities while reducing computational costs.</p>
<p><strong>BOHB: robust and efficient hyperparameter optimization at scale</strong><span id="id13">[<a class="reference internal" href="#id77" title="Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In ICML'18: Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, 1436–1445. PMLR, 2018.">1</a>]</span></p>
<p>In this method, the surrogate model uses a Tree-structured Parzen Estimator (TPE) to model the hyperparameter space. TPE builds separate probability models for good and bad configurations using kernel density estimation. The TPE model guides the search by maximizing the ratio between these models, effectively focusing on promising regions of the search space.</p>
</section>
<section id="acquisition-function">
<h3>Acquisition Function<a class="headerlink" href="#acquisition-function" title="Link to this heading">#</a></h3>
<p><strong>Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles</strong></p>
<p>In RGPE, the acquisition function follows standard BO methods but integrates the ranking-weighted ensemble model. The ensemble combines predictions from multiple GPs, each weighted based on its ranking performance in relation to the current task. The acquisition function then uses this weighted ensemble to balance exploration and exploitation, ensuring that the most relevant past models are given greater influence when selecting the next point to evaluate</p>
<p><strong>Scalable Gaussian process-based transfer surrogates for hyperparameter optimization</strong></p>
<p>This approach is referred to as the <em>transfer acquisition function</em> (TAF). The acquisition function balances exploration and exploitation by combining the predicted improvement from the new data with predicted improvements from previous tasks, weighted by their relevance. The weights are calculated the same as the model.</p>
<p><strong>Multi-Task Bayesian Optimization</strong></p>
<p>In this method, the acquisition function extends the standard EI criterion to the multi-task setting. It dynamically selects which task to evaluate by considering the correlation between tasks. The acquisition function maximizes information gain per unit cost by balancing the evaluation of cheaper auxiliary tasks with more expensive primary tasks, using the entropy search strategy.</p>
<p><strong>Multi-Fidelity Bayesian Optimization via Deep Neural Networks</strong></p>
<p>It aims to maximize the mutual information between the predicted maximum of the objective function and the next point to be evaluated. The acquisition function selects the input location and fidelity level that provide the highest benefit-cost ratio. By employing fidelity-wise moment matching and Gauss-Hermite quadrature to approximate the posterior distributions, the acquisition function ensures that both fidelity selection and input sampling are computationally efficient and well-informed.</p>
<p><strong>BOHB:Robust and Efficient Hyperparameter Optimization at Scale</strong></p>
<p>It selects new configurations by maximizing the expected improvement, using kernel density estimates of good and bad configurations. BOHB combines this with a multi-fidelity approach, which allows the acquisition function to operate across different budget levels, efficiently balancing exploration and exploitation while scaling to large optimization tasks</p>
<p><strong>Reinforced Few-Shot Acquisition Function Learning for Bayesian Optimization</strong><span id="id14">[<a class="reference internal" href="#id69" title="Bing-Jing Hsieh, Ping-Chun Hsieh, and Xi Liu. Reinforced few-shot acquisition function learning for Bayesian optimization. In NeurIPS'21: Proc. of the 34th Annual Conference on Neural Information Processing Systems, 7718–7731. 2021.">5</a>]</span></p>
<p>In this method, the acquisition function is modeled with a deep Q-network (DQN), learning to balance exploration and exploitation as a reinforcement learning task. The DQN predicts sampling utility based on the posterior mean and variance, refined by a Bayesian variant that incorporates uncertainty to avoid overfitting.</p>
</section>
</section>
<section id="list-of-algorithmic-objects">
<span id="alg-obj"></span><h2>List of Algorithmic Objects<a class="headerlink" href="#list-of-algorithmic-objects" title="Link to this heading">#</a></h2>
<p>The optimization framework includes a variety of state-of-the-art algorithms, each designed with specific features to address different classes of optimization problems. The table below provides a summary of the key algorithms available, categorized by their class, convenience for use, targeted objective(s), and any constraints they impose.</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 35.3%" />
<col style="width: 5.9%" />
<col style="width: 58.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithmic Objects</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Source Algorithm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Prune</p></td>
<td><p>Space Transform</p></td>
<td><p>Hyperparameter Search Space Pruning – A New Component for Sequential Model-Based Hyperparameter Optimization</p></td>
</tr>
<tr class="row-even"><td><p>Prune</p></td>
<td><p>Space Transform</p></td>
<td><p>Learning search spaces for Bayesian optimization- Another view of hyperparameter transfer learning</p></td>
</tr>
<tr class="row-odd"><td><p>EA-based initialization</p></td>
<td><p>Initialization Design</p></td>
<td><p>Few-Shot Bayesian Optimization with Deep Kernel Surrogates</p></td>
</tr>
<tr class="row-even"><td><p>MI-SMBO</p></td>
<td><p>Initialization Design</p></td>
<td><p>Initializing Bayesian Hyperparameter Optimization via Meta-Learning</p></td>
</tr>
<tr class="row-odd"><td><p>aLI</p></td>
<td><p>Initialization Design</p></td>
<td><p>Learning Hyperparameter Optimization Initializations</p></td>
</tr>
<tr class="row-even"><td><p>Prior-GP</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Pre-trained Gaussian processes for Bayesian optimization</p></td>
</tr>
<tr class="row-odd"><td><p>Deep kernel GP</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Few-Shot Bayesian Optimization with Deep Kernel Surrogates</p></td>
</tr>
<tr class="row-even"><td><p>MHGP</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Google Vizier-A Service for Black-Box Optimization</p></td>
</tr>
<tr class="row-odd"><td><p>PFNs</p></td>
<td><p>Surrogate Model</p></td>
<td><p>PFNs4BO- In-Context Learning for Bayesian Optimization</p></td>
</tr>
<tr class="row-even"><td><p>Weighted-sum Gaussian Process</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Scalable Gaussian process-based transfer surrogates for hyperparameter optimization</p></td>
</tr>
<tr class="row-odd"><td><p>Rank-based Gaussian Process</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles</p></td>
</tr>
<tr class="row-even"><td><p>Multi-task Gaussian Process</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Multi-Task Bayesian Optimization</p></td>
</tr>
<tr class="row-odd"><td><p>DNN-MF</p></td>
<td><p>Surrogate Model</p></td>
<td><p>Multi-Fidelity Bayesian Optimization via Deep Neural Networks</p></td>
</tr>
<tr class="row-even"><td><p>TPE</p></td>
<td><p>Surrogate Model</p></td>
<td><p>BOHB: robust and eﬀicient hyperparameter optimization at scale</p></td>
</tr>
<tr class="row-odd"><td><p>TAF</p></td>
<td><p>Acquisition Function</p></td>
<td><p>Scalable Gaussian process-based transfer surrogates for hyperparameter optimization</p></td>
</tr>
<tr class="row-even"><td><p>Rank-based ACF</p></td>
<td><p>Acquisition Function</p></td>
<td><p>Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles</p></td>
</tr>
<tr class="row-odd"><td><p>MX-Entropy</p></td>
<td><p>Acquisition Function</p></td>
<td><p>Multi-Task Bayesian Optimization</p></td>
</tr>
<tr class="row-even"><td><p>FSAF</p></td>
<td><p>Acquisition Function</p></td>
<td><p>Reinforced Few-Shot Acquisition Function Learning</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id15">
<div role="list" class="citation-list">
<div class="citation" id="id77" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">1</a><span class="fn-bracket">]</span></span>
<p>Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In <em>ICML'18: Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 1436–1445. PMLR, 2018.</p>
</div>
<div class="citation" id="id142" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">2</a><span class="fn-bracket">]</span></span>
<p>Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for Bayesian optimization using ranking-weighted Gaussian process ensembles. In <em>ICML 2018 AutoML Workshop</em>, volume 7, 1–15. 2018.</p>
</div>
<div class="citation" id="id66" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. In <em>AAAI'15: Proc. of the 2015 AAAI Conference on Artificial Intelligence</em>, 1128–1135. AAAI Press, 2015.</p>
</div>
<div class="citation" id="id88" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">4</a><span class="fn-bracket">]</span></span>
<p>Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. Google vizier: A service for black-box optimization. In <em>KDD'17: Proc. of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1487–1495. ACM, 2017.</p>
</div>
<div class="citation" id="id69" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">5</a><span class="fn-bracket">]</span></span>
<p>Bing-Jing Hsieh, Ping-Chun Hsieh, and Xi Liu. Reinforced few-shot acquisition function learning for Bayesian optimization. In <em>NeurIPS'21: Proc. of the 34th Annual Conference on Neural Information Processing Systems</em>, 7718–7731. 2021.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">6</a><span class="fn-bracket">]</span></span>
<p>Shibo Li, Wei W. Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In <em>NIPS'20: The Proc. of the 33th Advances in Neural Information Processing Systems</em>. 2020.</p>
</div>
<div class="citation" id="id171" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">7</a><span class="fn-bracket">]</span></span>
<p>Samuel Müller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: in-context learning for Bayesian optimization. In <em>ICML'23: Proc of the International Conference on Machine Learning</em>, volume 202, 25444–25470. PMLR, 2023.</p>
</div>
<div class="citation" id="id168" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">8</a><span class="fn-bracket">]</span></span>
<p>Valerio Perrone and Huibin Shen. Learning search spaces for Bayesian optimization: another view of hyperparameter transfer learning. In <em>NIPS'19: Proc of the 2019 Advances in Neural Information Processing Systems</em>, 12751–12761. 2019.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">9</a><span class="fn-bracket">]</span></span>
<p>Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task Bayesian optimization. In <em>NIPs'13: Proc of the 2013 Annual Conference on Neural Information Processing Systems</em>, 2004–2012. 2013.</p>
</div>
<div class="citation" id="id98" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">10</a><span class="fn-bracket">]</span></span>
<p>Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained Gaussian processes for Bayesian optimization. <em>arXiv preprint arXiv:2109.08215</em>, 2021.</p>
</div>
<div class="citation" id="id70" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">11</a><span class="fn-bracket">]</span></span>
<p>Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In <em>ICLR'21: Proc. of the 9th International Conference on Learning Representations</em>. OpenReview.net, 2021.</p>
</div>
<div class="citation" id="id167" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">12</a><span class="fn-bracket">]</span></span>
<p>Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Hyperparameter search space pruning - a new component for sequential model-based hyperparameter optimization. In <em>ECML/PKDD'15: Proc of the 2015 Advances in Machine Learning and Knowledge Discovery in Databases</em>, volume 9285, 104–119. 2015.</p>
</div>
<div class="citation" id="id67" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">13</a><span class="fn-bracket">]</span></span>
<p>Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization initializations. In <em>DSAA'15: Proc. of the 2015 IEEE International Conference on Data Science and Advanced Analytics</em>, 1–10. IEEE, 2015.</p>
</div>
<div class="citation" id="id97" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">14</a><span class="fn-bracket">]</span></span>
<p>Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable Gaussian process-based transfer surrogates for hyperparameter optimization. <em>Mach. Learn.</em>, 107(1):43–78, 2018.</p>
</div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../quickstart.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quick Start</p>
      </div>
    </a>
    <a class="right-next"
       href="problems.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Benchmark Problems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#registering-a-new-algorithm-in-transopt">Registering a New Algorithm in TransOPT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-algorithms">Supported Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#search-space-transform">Search space transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-design">Initialization Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogate-model">Surrogate Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acquisition-function">Acquisition Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#list-of-algorithmic-objects">List of Algorithmic Objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Peili Mao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, Peili Mao.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>